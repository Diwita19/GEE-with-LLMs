Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=True, eval_dataset_size=5, max_train_samples=150, max_eval_samples=250, source_max_len=512, target_max_len=256, dataset='/home/dbanerj/ondemand/data/sys/dashboard/batch_connect/sys/Code-Server/output/c4520397-dc96-4606-a46c-1772a7a796b0/NLP_Assignment_3/GEE-with-LLMs/data/fine-tune_data/German_train_test_data/500_train_atomic_edit_gold_w_seq.jsonl', dataset_format=None, output_dir='./finetuned_output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=250, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./finetuned_output/runs/Nov20_20-07-53_gpu020.orc.gmu.edu', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=2, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=1, past_index=-1, run_name='./finetuned_output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 512,
  "transformers_version": "4.30.2"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Loading base model meta-llama/Llama-2-7b-hf...
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:18<00:00,  9.27s/it]
Using pad_token, but it is not set yet.
Adding LoRA modules...
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Generated Prediction: Test input example
 everybody should know.


## Installation

Install [Rust](https://www.rust-lang.org/en-US/install.html) and use `cargo install` to install the `rust-cli
loaded model
Splitting dataset into train and validation...
Raw Sample Input: This is an atomic edit extraction task. Given a pair of German sentences and a list of edits, your task is to extract atomic edits and assign each edit a label. Be case sensitive. Pay attention to punctuation marks and relocated tokens (relocated tokens should not be changed before and after being relocated). Pay attention to phonetic similarity when aligning tokens.
Sentence pair:
Wohnungssituation in meinen Heimatstadt.
Die Wohnungssituation in meiner Heimatstadt.
list of edits:
('insert', '', 'Die')
('replace', 'meinen', 'meiner')
Raw Sample Output: ["insert", "", "Die"]
["replace", "meinen", "meiner"]
Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 1127.56 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 332.52 examples/s]
Tokenized Input IDs: [1, 910, 338, 385, 23489, 3863, 4805, 428, 3414, 29889, 11221, 263, 5101, 310, 5332, 25260, 322, 263, 1051, 310, 1226, 1169, 29892, 596, 3414, 338, 304, 6597, 23489, 1226, 1169, 322, 3566, 1269, 3863, 263, 3858, 29889, 1522, 1206, 20502, 29889, 14617, 8570, 304, 6035, 22999, 362, 17997, 322, 337, 28809, 18897, 313, 276, 28809, 18897, 881, 451, 367, 3939, 1434, 322, 1156, 1641, 337, 28809, 467, 14617, 8570, 304, 1374, 265, 7492, 29501, 746, 7595, 292, 18897, 29889, 13, 29903, 296, 663, 5101, 29901, 13, 29956, 6547, 686, 893, 1981, 362, 297, 592, 7026, 22200, 9516, 29889, 13, 16334, 21119, 686, 893, 1981, 362, 297, 592, 4983, 22200, 9516, 29889, 13, 1761, 310, 1226, 1169, 29901, 13, 877, 7851, 742, 15516, 525, 16334, 1495, 13, 877, 6506, 742, 525, 1004, 7026, 742, 525, 1004, 4983, 1495, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]
Tokenized Input Decoded: <s>This is an atomic edit extraction task. Given a pair of German sentences and a list of edits, your task is to extract atomic edits and assign each edit a label. Be case sensitive. Pay attention to punctuation marks and relocated tokens (relocated tokens should not be changed before and after being relocated). Pay attention to phonetic similarity when aligning tokens.
Sentence pair:
Wohnungssituation in meinen Heimatstadt.
Die Wohnungssituation in meiner Heimatstadt.
list of edits:
('insert', '', 'Die')
('replace', 'meinen', 'meiner') [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
Tokenized Output IDs: [1, 6796, 7851, 613, 12633, 376, 16334, 3108, 13, 3366, 6506, 613, 376, 1004, 7026, 613, 376, 1004, 4983, 3108, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]
Tokenized Output Decoded: <s>["insert", "", "Die"]
["replace", "meinen", "meiner"] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205
torch.float32 422326272 0.11537932153507864
torch.uint8 3238002688 0.8846206784649213
  0%|                                                                                                                                  | 0/250 [00:00<?, ?it/s]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 75%|█████████████████████████████████████████████████████████████████████████████████████████▊                              | 187/250 [31:02<10:44, 10.24s/it]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.1912, 'learning_rate': 0.0002, 'epoch': 2.67}
{'loss': 0.0355, 'learning_rate': 0.0002, 'epoch': 5.33}
{'loss': 0.0142, 'learning_rate': 0.0002, 'epoch': 8.0}
  return fn(*args, **kwargs)                                                                                                                                   
Raw Predictions (Token IDs): [[396, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 372, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 6796, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 376, 526, 367, 367, 9859, 467, 1641, 1156, 278, 337, 28809, 467, 6796, 8570, 304, 278, 265, 7492, 12651, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 29901, 589, 9016, 479, 297, 589, 29889, 9016, 300, 297, 4093, 12406, 6796, 1727, 29889, 518, 7975, 11529, 29895, 783, 3072, 336, 3072, 284, 2724, 29889, 762, 301, 1173, 29889, 29889, 29889, 4028, 29889, 279, 1620, 2596, 300, 2256, 29889, 9016, 2579, 29889, 471, 11820, 9016, 311, 7975, 3072, 589, 2101, 10451, 1527, 927, 5267, 19457, 29873, 29889, 6796, 3366, 495, 7975, 454, 915, 297, 10450, 563, 19221, 915, 568, 2862, 589, 869, 11121, 29892, 24107, 7975, 25424, 25424, 1276, 29895, 2010, 29634, 367, 414, 1173, 563, 4493, 26743, 1173, 9083, 9083, 7975, 527, 24014, 17846, 279, 2505, 2596, 300, 2256, 563, 7655, 2579, 29892, 553, 11820, 2949, 311, 7975, 1005, 1011, 2101, 14152, 1527, 367, 5267, 19457, 29873, 29889, 6796, 3366, 6796, 1226, 1169, 6796, 6796, 3366, 23536, 742, 376, 16217, 300, 742, 376, 23536, 568, 2033, 6796, 3366, 23536, 613, 11393, 742, 20569, 6796, 3366, 6506, 742, 376, 29933, 1276, 29895, 8403, 29886, 19690, 742, 376, 29911, 260, 1276, 29895, 2010, 29634, 3108, 6796, 3366, 6506, 613, 12633, 376, 1495, 6796, 6506, 613, 376, 15564, 300, 613, 376, 23536, 568, 3108, 13, 3366, 8143, 613, 11393, 613, 376, 3108, 13, 3366, 6506, 613, 12633, 28796, 16217, 3108, 13, 3366, 6506, 613, 376, 29911, 1276, 29895, 8403, 29886, 19690, 613, 376, 29873, 1276, 29895, 2010, 29634, 3108, 13, 3366, 7851, 613, 12633, 28796, 29962, 13, 1], [396, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 372, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 6796, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 376, 526, 367, 367, 9859, 467, 1641, 1156, 278, 337, 28809, 467, 6796, 8570, 304, 278, 265, 7492, 12651, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 294, 831, 1752, 3072, 805, 329, 29892, 6796, 1729, 15857, 1752, 1729, 24152, 29889, 6796, 3366, 2108, 831, 1752, 1729, 425, 329, 29889, 563, 762, 23115, 1752, 1729, 27568, 29889, 6796, 3366, 6796, 1226, 1169, 6796, 6796, 3366, 277, 6796, 376, 742, 28796, 1159, 6796, 6506, 613, 11393, 613, 28796, 29962, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [396, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 372, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 6796, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 376, 526, 367, 367, 9859, 467, 1641, 1156, 278, 337, 28809, 467, 6796, 8570, 304, 278, 265, 7492, 12651, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 305, 9016, 1011, 29896, 29900, 29896, 5272, 6796, 19221, 9016, 10883, 21488, 1865, 762, 22257, 29899, 20547, 29899, 1808, 29889, 972, 29889, 6796, 3366, 305, 9016, 29871, 29906, 29900, 3866, 5272, 563, 7975, 26636, 10883, 21488, 1865, 2128, 8407, 29899, 18784, 624, 855, 1808, 297, 10450, 29889, 6796, 3366, 6796, 1226, 1169, 6796, 6796, 3366, 29902, 376, 376, 24939, 5610, 7
Raw Labels (Token IDs): [[6796, 6506, 613, 376, 15564, 300, 613, 376, 23536, 568, 3108, 13, 3366, 8143, 613, 11393, 613, 376, 3108, 13, 3366, 7851, 613, 12633, 376, 16217, 3108, 13, 3366, 6506, 613, 376, 29911, 1276, 29895, 8403, 29886, 19690, 613, 376, 29873, 1276, 29895, 2010, 29634, 3108, 13, 3366, 7851, 613, 12633, 28796, 29962, 2], [6796, 6506, 613, 11393, 613, 28796, 29962, 2], [6796, 6506, 613, 376, 29967, 5610, 613, 376, 29967, 801, 276, 3108, 13, 3366, 6506, 613, 376, 24528, 613, 376, 1997, 3108, 13, 3366, 6506, 613, 376, 29909, 29884, 29899, 18784, 624, 1808, 613, 376, 29909, 29884, 29899, 18784, 29899, 855, 1808, 3108, 2], [6796, 7851, 613, 12633, 28796, 29962, 13, 3366, 8143, 613, 376, 24195, 613, 376, 3108, 2], [6796, 6506, 613, 376, 1145, 613, 376, 672, 3108, 13, 3366, 6506, 613, 376, 29968, 4227, 332, 613, 376, 29968, 9730, 3108, 2]]
Sample Predictions (Decoded): ['# is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, it task is to extract the edits from their them edit to type from sure-. [" attention to thectuation.. whitespaceorder words.eloc " are be be assigned). being after the relocated). [" attention to theonetic differences. reing words. ["["entence # [" ["[": der binge in der. binet in einem Deutschen ["reg. [ ich dortkisch nichtra nichtalchen. die lche... Aus.ar alsbersetzen. binaten. washalb binde ich nicht derigen Kolmenangevorzugt. ["["ber ich lebe in Deutschland und habebeite bei der .bahn, weil ich Tür Türürkische Sprache beersche und Deutsch spreche kann kann ich im Unternehmen sogar übersetzen und beraten, deshalb werde ich von einigen Firmen bevorzugt. ["[" [" edits [" ["["arbe\', "dieet\', "arbeite\'] ["["arbe", ".\', "") ["["replace\', "Bürkischesprache\', "T türkische Sprache"] ["["replace", "", "\') ["replace", "arbeitet", "arbeite"] ["delete", ".", ""] ["replace", "", ","die"] ["replace", "Türkischesprache", "türkische Sprache"] ["insert", "", ","]', '# is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, it task is to extract the edits from their them edit to type from sure-. [" attention to thectuation.. whitespaceorder words.eloc " are be be assigned). being after the relocated). [" attention to theonetic differences. reing words. ["["entence # [" ["["as es ist nicht sput, [" zu Kinder ist zu klein. ["["enn es ist zu laut. und die Mitte ist zu hoch. ["[" [" edits [" ["["it [" "\', ","") ["replace", ".", ","]', '# is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, it task is to extract the edits from their them edit to type from sure-. [" attention to thectuation.. whitespaceorder words.eloc " are be be assigned). being after the relocated). [" attention to theonetic differences. reing words. ["["entence # [" ["["ch bin ein101 alt [" habe biniere mich für die Karriere-Pair-elle. den. ["["ch bin 20 Jahr alt und ich interessiere mich für eine Au-pair StStelle in Deutschland. ["[" [" edits [" ["["I " "jahrahr\',\', "Jahre Alt"] ["["insert", "interu-pair\',elle\', "Au-pair-Stelle"] ["replace", "Jahr", "Jahre"] ["replace", "Alt", "alt"] ["replace", "Au-pair",elle", "Au-pair-Stelle"]', '# is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, it task is to extract the edits from their them edit to type from sure-. [" attention to thectuation.. whitespaceorder words.eloc " are be be assigned). being after the relocated). [" attention to theonetic differences. reing words. ["["entence # [" ["["ch binöchte, neueüss ung.. ich viel2 Kinder habe. ["["ch möchte eine gr Wohnung sein weil ich 4 Kinder habe. ["[" [" edits [" ["["insert\', "größe\', "große\'] ["["insert", "4\', " weil ["replace", "", ","] ["replace", "gr", ""]', '# is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, it task is to extract the edits from their them edit to type from sure-. [" attention to thectuation.. whitespaceorder words.eloc " are be be assigned). being after the relocated). [" attention to theonetic differences. reing words. ["["entence # [" ["["ch binke, dass es uns S nicht dem Bü Vzenauf.ernen sol. ["["ch denke, dass wir die Geschichte aus den Kur kennen lernen können. ["[" [" edits [" ["["den " "diekeur\', "den Kultur\'] ["replace", "den", "der"] ["replace", "Kurtur", "Kultur"]']
Sample Labels (Decoded): ['["replace", "arbeitet", "arbeite"] ["delete", ".", ""] ["insert", "", "die"] ["replace", "Türkischesprache", "türkische Sprache"] ["insert", "", ","]', '["replace", ".", ","]', '["replace", "Jahr", "Jahre"] ["replace", "Alt", "alt"] ["replace", "Au-pair Stelle", "Au-pair-Stelle"]', '["insert", "", ","] ["delete", "sein", ""]', '["replace", "den", "der"] ["replace", "Kurtur", "Kultur"]']
{'eval_loss': 0.5446541905403137, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.6059, 'eval_samples_per_second': 1.919, 'eval_steps_per_second': 1.919, 'epoch': 9.97}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [41:29<00:00,  9.93s/it]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
{'loss': 0.0122, 'learning_rate': 0.0002, 'epoch': 10.67}
{'loss': 0.0052, 'learning_rate': 0.0002, 'epoch': 13.33}
  warnings.warn(
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Saving PEFT checkpoint...
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [41:36<00:00,  9.99s/it]
{'train_runtime': 2496.5457, 'train_samples_per_second': 0.801, 'train_steps_per_second': 0.1, 'train_loss': 0.051671683549880984, 'epoch': 13.33}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =      13.33
  train_loss               =     0.0517
  train_runtime            = 0:41:36.54
  train_samples_per_second =      0.801
  train_steps_per_second   =        0.1
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.56it/s]
Raw Predictions (Token IDs): [[29871, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 445, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 9859, 467, 1641, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 29901, 762, 9016, 479, 297, 589, 29889, 9016, 300, 297, 4093, 12406, 6796, 1727, 29889, 518, 7975, 831, 29895, 783, 3072, 336, 3072, 284, 2570, 29889, 762, 301, 1173, 29889, 29889, 29889, 4028, 22341, 279, 1620, 2596, 300, 2256, 29889, 9016, 2579, 29889, 471, 11820, 9016, 311, 7975, 3072, 589, 2101, 10451, 1527, 927, 5267, 19457, 29873, 29889, 6796, 3366, 495, 7975, 454, 915, 297, 10450, 563, 564, 915, 568, 2862, 589, 869, 11121, 29892, 24107, 7975, 25424, 25424, 1276, 29895, 2010, 29634, 367, 414, 1173, 563, 4493, 26743, 1173, 9083, 9083, 7975, 527, 24014, 17846, 279, 2505, 2596, 300, 2256, 563, 7655, 2579, 29892, 553, 11820, 2949, 311, 7975, 1005, 1011, 2101, 14152, 1527, 367, 5267, 19457, 29873, 29889, 6796, 3366, 6796, 1226, 1169, 6796, 6796, 3366, 276, 742, 376, 436, 300, 742, 376, 23536, 568, 2033, 6796, 3366, 6506, 613, 376, 613, 20569, 6796, 3366, 6506, 742, 376, 29933, 1276, 29895, 8403, 29886, 19690, 742, 376, 29911, 25424, 1276, 29895, 2010, 29634, 3108, 6796, 3366, 6506, 613, 12633, 376, 1159, 6796, 6506, 613, 376, 15564, 300, 613, 376, 23536, 568, 3108, 13, 3366, 8143, 613, 11393, 613, 376, 3108, 13, 3366, 6506, 613, 12633, 376, 16217, 3108, 13, 3366, 6506, 613, 376, 29911, 1276, 29895, 8403, 29886, 19690, 613, 376, 29873, 1276, 29895, 2010, 29634, 3108, 13, 3366, 7851, 613, 12633, 28796, 29962, 2, 1], [29871, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 445, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 9859, 467, 1641, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 294, 831, 1752, 3072, 805, 329, 29892, 6796, 1729, 15857, 1752, 1729, 24152, 29889, 6796, 3366, 2108, 831, 1752, 1729, 425, 329, 29889, 563, 762, 23115, 1752, 1729, 27568, 29889, 6796, 3366, 310, 1226, 1169, 6796, 6796, 3366, 276, 742, 376, 742, 525, 1159, 6796, 6506, 613, 11393, 613, 28796, 29962, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [29871, 338, 263, 29192, 29899, 310, 428, 310, 29889, 23196, 263, 1051, 310, 23489, 25260, 6796, 263, 9750, 310, 23489, 1169, 29892, 445, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 1134, 515, 2, 1854, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 9859, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 337, 292, 3838, 29889, 6796, 3366, 296, 663, 396, 6796, 6796, 3366, 305, 9016, 1011, 29896, 29900, 29896, 5272, 6796, 19221, 9016, 10883, 21488, 1865, 762, 22257, 29899, 20547, 17163, 1808, 297, 972, 29889, 6796, 3366, 626, 9016, 29871, 29906, 29900, 3866, 5272, 563, 7975, 26636, 10883, 21488, 1865, 2128, 8407, 29899, 18784, 624, 855, 1808, 297, 10450, 29889, 6796, 3366, 6796, 1226, 1169, 6796, 6796, 3366, 29902, 742, 376, 2109, 5610, 742, 742, 376, 29967, 5610, 2
Raw Labels (Token IDs): [[6796, 6506, 613, 376, 15564, 300, 613, 376, 23536, 568, 3108, 13, 3366, 8143, 613, 11393, 613, 376, 3108, 13, 3366, 7851, 613, 12633, 376, 16217, 3108, 13, 3366, 6506, 613, 376, 29911, 1276, 29895, 8403, 29886, 19690, 613, 376, 29873, 1276, 29895, 2010, 29634, 3108, 13, 3366, 7851, 613, 12633, 28796, 29962, 2], [6796, 6506, 613, 11393, 613, 28796, 29962, 2], [6796, 6506, 613, 376, 29967, 5610, 613, 376, 29967, 801, 276, 3108, 13, 3366, 6506, 613, 376, 24528, 613, 376, 1997, 3108, 13, 3366, 6506, 613, 376, 29909, 29884, 29899, 18784, 624, 1808, 613, 376, 29909, 29884, 29899, 18784, 29899, 855, 1808, 3108, 2], [6796, 7851, 613, 12633, 28796, 29962, 13, 3366, 8143, 613, 376, 24195, 613, 376, 3108, 2], [6796, 6506, 613, 376, 1145, 613, 376, 672, 3108, 13, 3366, 6506, 613, 376, 29968, 4227, 332, 613, 376, 29968, 9730, 3108, 2]]
Sample Predictions (Decoded): ['is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, this task is to extract the edits from their them edit to type from sure-. attention to thectuation.. whitespaceorder words.eloc tokens are be be assigned). being after the relocated). attention to theonetic trans. reing words. ["["entence # [" ["[": die binge in der. binet in einem Deutschen ["reg. [ ich eskisch nichtra nichtalcht. die lche... Aus gutar alsbersetzen. binaten. washalb binde ich nicht derigen Kolmenangevorzugt. ["["ber ich lebe in Deutschland und arbeite bei der .bahn, weil ich Tür Türürkische Sprache beersche und Deutsch spreche kann kann ich im Unternehmen sogar übersetzen und beraten, deshalb werde ich von einigen Firmen bevorzugt. ["[" [" edits [" ["["re\', "ichet\', "arbeite\'] ["["replace", "", "") ["["replace\', "Bürkischesprache\', "T Türürkische Sprache"] ["["replace", "", "") ["replace", "arbeitet", "arbeite"] ["delete", ".", ""] ["replace", "", "die"] ["replace", "Türkischesprache", "türkische Sprache"] ["insert", "", ","]', 'is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, this task is to extract the edits from their them edit to type from sure-. attention to thectuation.. whitespaceorder words.eloc tokens are be be assigned). being after the relocated). attention to theonetic trans. reing words. ["["entence # [" ["["as es ist nicht sput, [" zu Kinder ist zu klein. ["["enn es ist zu laut. und die Mitte ist zu hoch. ["[" of edits [" ["["re\', "\', \'") ["replace", ".", ","]', 'is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, this task is to extract the edits from their them edit to type from sure-. attention to thectuation.. whitespaceorder words.eloc tokens are be be assigned). extra after the relocated). attention to theonetic trans. reing words. ["["entence # [" ["["ch bin ein101 alt [" habe biniere mich für die Karriere-Pair Jobelle in den. ["[" am bin 20 Jahr alt und ich interessiere mich für eine Au-pair StStelle in Deutschland. ["[" [" edits [" ["["I\', "binahr\',\', "Jahrre Alt"] ["["replace\', "interu-pair\',elle\', "Au-pair-Stelle"] ["replace", "Jahr", "Jahre"] ["replace", "Alt", "alt"] ["replace", "Au-pair",elle", "Au-pair-Stelle"]', 'is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, this task is to extract the edits from their them edit to type from sure-. attention to thectuation.. whitespaceorder words.eloc tokens are be be assigned). being after the relocated). attention to theonetic trans. reing words. ["["entence # [" ["["ch binöchte, neueüße ung.. ich ein2 Kinder habe [" ["[" want möchte eine gr Wohnung sein weil ich 4 Kinder habe. ["[" of edits [" ["["insert [" "größe\', "große"] ["["replace", "hab\', " weil ["replace", "", ","] ["replace", "gr", ""]', 'is a exclusive- ofction of. nobody a list of atomic sentences [" a verb of atomicits, this task is to extract the edits from their them edit to type from sure-. attention to thectuation.. whitespaceorder words.eloc tokens are be be assigned). extra after the relocated). attention to theonetic trans. reing words. ["["entence # [" ["["ch binke, dass es uns S nicht dem Bü Vzenauf.ernen soll. ["[" think denke, dass wir die Geschichte aus den Kur kennen lernen können. ["[" [" edits [" ["["re [" "Kkeur\', "den Kultur"] ["replace", "den", "der"] ["replace", "Kurtur", "Kultur"]']
Sample Labels (Decoded): ['["replace", "arbeitet", "arbeite"] ["delete", ".", ""] ["insert", "", "die"] ["replace", "Türkischesprache", "türkische Sprache"] ["insert", "", ","]', '["replace", ".", ","]', '["replace", "Jahr", "Jahre"] ["replace", "Alt", "alt"] ["replace", "Au-pair Stelle", "Au-pair-Stelle"]', '["insert", "", ","] ["delete", "sein", ""]', '["replace", "den", "der"] ["replace", "Kurtur", "Kultur"]']
Processed Predictions (Sample): ['replace", "arbeitet", "arbeite"]\n["delete", ".", ""]\n["replace", "", "die"]\n["replace", "Türkischesprache", "türkische Sprache"]\n["insert", "", ","]', 'replace", ".", ","]', 'replace", "Jahr", "Jahre"]\n["replace", "Alt", "alt"]\n["replace", "Au-pair",elle", "Au-pair-Stelle"]', 'replace", "", ","]\n["replace", "gr", ""]', 'replace", "den", "der"]\n["replace", "Kurtur", "Kultur"]']
Processed Labels (Sample): ['["replace", "arbeitet", "arbeite"]\n["delete", ".", ""]\n["insert", "", "die"]\n["replace", "Türkischesprache", "türkische Sprache"]\n["insert", "", ","]', '["replace", ".", ","]', '["replace", "Jahr", "Jahre"]\n["replace", "Alt", "alt"]\n["replace", "Au-pair Stelle", "Au-pair-Stelle"]', '["insert", "", ","]\n["delete", "sein", ""]', '["replace", "den", "der"]\n["replace", "Kurtur", "Kultur"]']
Precision: 0.0 Recall: 0.0 F1: 0.0
***** eval metrics *****
  eval_f1        = 0.0
  eval_precision = 0.0
  eval_recall    = 0.0
