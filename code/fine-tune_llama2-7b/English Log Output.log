Namespace(model_name_or_path='meta-llama/Llama-2-7b-hf', trust_remote_code=False, use_auth_token=True, eval_dataset_size=5, max_train_samples=150, max_eval_samples=250, source_max_len=512, target_max_len=256, dataset='/home/dbanerj/ondemand/data/sys/dashboard/batch_connect/sys/Code-Server/output/c4520397-dc96-4606-a46c-1772a7a796b0/NLP_Assignment_3/GEE-with-LLMs/data/fine-tune_data/English_train_test_data/training.jsonl', dataset_format=None, output_dir='./finetuned_output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=1, per_device_eval_batch_size=1, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=8, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=250, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./finetuned_output/runs/Nov20_21-19-45_gpu021.orc.gmu.edu', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=250, save_total_limit=2, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=187, dataloader_num_workers=1, past_index=-1, run_name='./finetuned_output', disable_tqdm=False, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {
  "max_new_tokens": 512,
  "transformers_version": "4.30.2"
}
, cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16.0, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
, _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)
Loading base model meta-llama/Llama-2-7b-hf...
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:22<00:00, 11.14s/it]
Using pad_token, but it is not set yet.
Adding LoRA modules...
Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.
Generated Prediction: Test input example:
 surely_no_one_will_ever_need_to_write_this_much_text

#include <stdio.h>
#include <stdlib.h>
#include <string.h>
loaded model
Generating train split: 512 examples [00:00, 8054.29 examples/s]
Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:00<00:00, 10049.95 examples/s]
Splitting dataset into train and validation...
Raw Sample Input: This is an atomic edit extraction task. Given a pair of English sentences and a list of edits, your task is to extract atomic edits and assign each edit a label. Be case sensitive. Pay attention to punctuation marks and relocated tokens (relocated tokens should not be changed before and after being relocated). Pay attention to phonetic similarity when aligning tokens.
Sentence pair:
I am flexible, a good team player, and capable to work under pressure.
I am flexible, a good team player, and able to work under pressure.
list of edits:
('replace', 'capable', 'able')
Raw Sample Output: ['replace', 'capable', 'able']
Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 1182.76 examples/s]
Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 401.16 examples/s]
Tokenized Input IDs: [1, 910, 338, 385, 23489, 3863, 4805, 428, 3414, 29889, 11221, 263, 5101, 310, 4223, 25260, 322, 263, 1051, 310, 1226, 1169, 29892, 596, 3414, 338, 304, 6597, 23489, 1226, 1169, 322, 3566, 1269, 3863, 263, 3858, 29889, 1522, 1206, 20502, 29889, 14617, 8570, 304, 6035, 22999, 362, 17997, 322, 337, 28809, 18897, 313, 276, 28809, 18897, 881, 451, 367, 3939, 1434, 322, 1156, 1641, 337, 28809, 467, 14617, 8570, 304, 1374, 265, 7492, 29501, 746, 7595, 292, 18897, 29889, 13, 29903, 296, 663, 5101, 29901, 13, 29902, 626, 25706, 29892, 263, 1781, 3815, 4847, 29892, 322, 15390, 304, 664, 1090, 12959, 29889, 13, 29902, 626, 25706, 29892, 263, 1781, 3815, 4847, 29892, 322, 2221, 304, 664, 1090, 12959, 29889, 13, 1761, 310, 1226, 1169, 29901, 13, 877, 6506, 742, 525, 5030, 519, 742, 525, 519, 1495, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]
Tokenized Input Decoded: <s>This is an atomic edit extraction task. Given a pair of English sentences and a list of edits, your task is to extract atomic edits and assign each edit a label. Be case sensitive. Pay attention to punctuation marks and relocated tokens (relocated tokens should not be changed before and after being relocated). Pay attention to phonetic similarity when aligning tokens.
Sentence pair:
I am flexible, a good team player, and capable to work under pressure.
I am flexible, a good team player, and able to work under pressure.
list of edits:
('replace', 'capable', 'able') [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
Tokenized Output IDs: [1, 6024, 6506, 742, 525, 5030, 519, 742, 525, 519, 2033, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000, 32000]
Tokenized Output Decoded: <s>['replace', 'capable', 'able'] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205
torch.float32 422326272 0.11537932153507864
torch.uint8 3238002688 0.8846206784649213
  0%|                                                                                                                        | 0/250 [00:00<?, ?it/s]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 75%|██████████████████████████████████████████████████████████████████████████████████▎                           | 187/250 [30:12<10:01,  9.55s/it]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.227, 'learning_rate': 0.0002, 'epoch': 2.67}
{'loss': 0.052, 'learning_rate': 0.0002, 'epoch': 5.33}
{'loss': 0.0212, 'learning_rate': 0.0002, 'epoch': 8.0}
  return fn(*args, **kwargs)                                                                                                                         
Raw Predictions (Token IDs): [[23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 16743, 292, 25260, 29889, 2, 1839, 296, 663, 29871, 29901, 518, 1839, 306, 6575, 471, 29892, 3512, 23407, 306, 7284, 750, 750, 297, 297, 2, 10401, 278, 13135, 7743, 29892, 306, 7091, 763, 263, 11199, 29892, 28472, 29889, 2, 1839, 29918, 1226, 1169, 29901, 2, 1839, 10401, 742, 525, 525, 2033, 6024, 1839, 6506, 742, 525, 535, 742, 525, 6024, 7851, 742, 15516, 13420, 2033, 13, 1839, 7851, 742, 525, 4716, 742, 525, 2033, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 16743, 292, 25260, 29889, 2, 1839, 296, 663, 29871, 29901, 518, 1839, 29901, 278, 293, 304, 278, 508, 6597, 2894, 29892, 591, 29892, 528, 2712, 29889, 29889, 2, 1762, 297, 6388, 293, 29892, 495, 728, 29892, 591, 508, 1207, 19548, 19548, 29892, 263, 3056, 322, 916, 6433, 29879, 2, 1839, 29918, 1226, 1169, 29901, 13, 1839, 262, 742, 525, 525, 29874, 431, 29890, 728, 2033, 6024, 1839, 6506, 742, 15516, 525, 23156, 1495, 6024, 1839, 6506, 742, 15516, 525, 2455, 1495, 6024, 1839, 6506, 742, 525, 262, 3096, 742, 742, 525, 303, 3096, 2033, 6024, 7851, 742, 15516, 525, 29878, 431, 29890, 728, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 6506, 742, 303, 3096, 29879, 3788, 303, 3096, 2033, 2, 1], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 16743, 292, 25260, 29889, 2, 1839, 296, 663, 29871, 29901, 518, 1839, 278, 29892, 278, 29892, 278, 1058, 884, 373, 278, 1422, 1319, 5855, 23136, 6048, 29889, 526, 526, 304, 679, 1009, 1009, 664, 29889, 1797, 333, 1036, 29889, 2, 797, 6124, 304, 445, 29892, 2305, 526, 1985, 297, 1407, 22884, 1319, 664, 29886, 6048, 322, 896, 864, 304, 5967, 515, 4195, 297, 8753, 333, 1036, 29889, 2, 1839, 29918, 1226, 1169, 29901, 13, 1839, 1366, 742, 525, 1366, 742, 525, 6024, 1839, 6506, 742, 525, 1366, 742, 525, 29881, 3864, 2033, 6024, 8143, 742, 525, 3166, 742, 525, 2033, 13, 1839, 6506, 742, 525, 262, 742, 29881, 3864, 2033, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 1301, 29889, 16743, 292, 25260, 29889, 2, 1839, 296, 663, 29871, 29901, 518, 1839, 29915, 263, 1900, 310, 306, 471, 304, 3377, 292, 29889, 2, 7058, 471, 278, 5828, 2020, 2020, 306, 748, 15007, 3377, 292, 29889, 2, 1839, 310, 1226, 1169, 29901, 2, 1839, 7058, 742, 525, 24098, 742, 525, 275, 203
Raw Labels (Token IDs): [[6024, 7851, 742, 15516, 13420, 2033, 13, 1839, 8143, 742, 525, 4716, 742, 525, 2033, 2], [6024, 7851, 742, 15516, 525, 29878, 431, 29890, 728, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 6506, 3788, 303, 3096, 29879, 3788, 303, 3096, 2033, 2], [6024, 8143, 742, 525, 3166, 742, 525, 2033, 13, 1839, 6506, 742, 525, 262, 3788, 29881, 3864, 2033, 2], [6024, 6506, 742, 525, 11102, 742, 525, 275, 2033, 13, 1839, 7851, 742, 15516, 525, 974, 2033, 2], [6024, 8143, 742, 525, 1552, 742, 525, 2033, 13, 1839, 6506, 742, 525, 275, 742, 525, 598, 2033, 2]]
Sample Predictions (Decoded): ["nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic trans. dealinging sentences.['entence : [[' I sun was, went tired I million had had in inWhen the concert finished, I felt like a bird, freely.['_ edits:['When', ' ''] ['['replace', 'con', ' ['insert', '', ','] ['insert', 'which', '']", "nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic trans. dealinging sentences.['entence : [[': theic to the can extract organ, we, sh things..To inorganic,berish, we can make bag bag, a hat and other stuffs['_ edits: ['in', ' 'aubbish'] ['['replace', '', 'bag') ['['replace', '', 'hat') ['['replace', 'inuff',', 'stuff'] ['insert', '', 'rubbish'] ['insert', '', 'a'] ['insert', '', 'a'] ['replace',stuffs','stuff']", "nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic trans. dealinging sentences.['entence : [[' the, the, the who also on the differentful conditions environmentslaces. are are to get their their work. orderidays.In addition to this, people are working in very stressful workplaces and they want to leave from condition in holidays.['_ edits: ['this', 'this', ' ['['replace', 'this', 'during'] ['delete', 'from', ''] ['replace', 'in',during']", "nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic trans. dealinging sentences.['entence : [['' a best of I was toboarding.That was the story why why I go snowboarding.[' of edits:['That', 'story', 'is'] ['['replace', '', 'the') ['replace', 'was', 'is'] ['insert', '', 'of']", "nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic trans. dealinging sentences.['entence : [[' year not least least, I transportation people to travel things things while they not for they.Last but not the, public transport allows people to do other things which is impossible when driving.['_ edits:['Last', 'the', ' ['['replace', 'not', 'are'] ['delete', 'the', ''] ['replace', 'is', 'are']"]
Sample Labels (Decoded): ["['insert', '', ','] ['delete', 'which', '']", "['insert', '', 'rubbish'] ['insert', '', 'a'] ['insert', '', 'a'] ['replace','stuffs','stuff']", "['delete', 'from', ''] ['replace', 'in','during']", "['replace', 'was', 'is'] ['insert', '', 'of']", "['delete', 'the', ''] ['replace', 'is', 'are']"]
{'eval_loss': 0.23772938549518585, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.5085, 'eval_samples_per_second': 1.993, 'eval_steps_per_second': 1.993, 'epoch': 9.97}
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [40:22<00:00,  9.79s/it]/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
{'loss': 0.0133, 'learning_rate': 0.0002, 'epoch': 10.67}
{'loss': 0.0046, 'learning_rate': 0.0002, 'epoch': 13.33}
  warnings.warn(
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Saving PEFT checkpoint...
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/dbanerj/miniconda3/envs/nlpenv/lib/python3.9/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 250/250 [40:29<00:00,  9.72s/it]
{'train_runtime': 2429.2454, 'train_samples_per_second': 0.823, 'train_steps_per_second': 0.103, 'train_loss': 0.06362341099977493, 'epoch': 13.33}
Saving PEFT checkpoint...
***** train metrics *****
  epoch                    =      13.33
  train_loss               =     0.0636
  train_runtime            = 0:40:29.24
  train_samples_per_second =      0.823
  train_steps_per_second   =      0.103
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  2.60it/s]
Raw Predictions (Token IDs): [[23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 3620, 29889, 16743, 292, 25260, 29889, 2, 29914, 296, 663, 29871, 29901, 518, 3366, 306, 6575, 471, 29892, 3512, 1407, 306, 7284, 750, 750, 297, 297, 2, 10401, 278, 13135, 7743, 29892, 306, 7091, 763, 263, 11199, 29892, 28472, 29889, 2, 1839, 29918, 1226, 1169, 29901, 2, 1839, 10401, 742, 525, 525, 2033, 6024, 1839, 6506, 742, 525, 535, 742, 525, 6024, 7851, 742, 15516, 13420, 2033, 13, 1839, 7851, 742, 525, 4716, 742, 525, 2033, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 3620, 29889, 16743, 292, 25260, 29889, 2, 29914, 296, 663, 29871, 29901, 518, 3366, 29901, 278, 293, 304, 278, 508, 6597, 2894, 29892, 541, 29892, 528, 2712, 29889, 29889, 2, 1762, 297, 6388, 293, 29892, 495, 728, 29892, 591, 508, 1207, 19548, 19548, 29892, 3056, 3056, 322, 916, 6433, 29879, 2, 1839, 29918, 1226, 1169, 29901, 13, 1839, 262, 742, 525, 525, 29874, 431, 29890, 728, 2033, 6024, 1839, 6506, 742, 15516, 525, 29874, 1495, 6024, 1839, 6506, 742, 15516, 525, 23156, 1495, 6024, 1839, 6506, 742, 525, 262, 3096, 742, 742, 525, 303, 3096, 2033, 6024, 7851, 742, 15516, 525, 29878, 431, 29890, 728, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 6506, 742, 303, 3096, 29879, 3788, 303, 3096, 2033, 2, 1], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 3620, 29889, 16743, 292, 25260, 29889, 2, 29914, 296, 663, 29871, 29901, 518, 3366, 278, 29892, 278, 29892, 278, 526, 884, 373, 278, 1422, 1319, 5855, 23136, 6048, 29889, 526, 526, 304, 679, 1009, 1009, 664, 29889, 1797, 333, 1036, 29889, 2, 797, 6124, 304, 445, 29892, 2305, 526, 1985, 297, 1407, 22884, 1319, 664, 29886, 6048, 322, 896, 864, 304, 5967, 515, 4195, 297, 8753, 333, 1036, 29889, 2, 1839, 29918, 1226, 1169, 29901, 13, 1839, 1366, 742, 525, 1366, 742, 525, 6024, 1839, 6506, 742, 525, 1366, 742, 525, 29881, 3864, 2033, 6024, 8143, 742, 525, 3166, 742, 525, 2033, 13, 1839, 6506, 742, 525, 262, 742, 29881, 3864, 2033, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23196, 338, 263, 29192, 13585, 310, 428, 5780, 29889, 23196, 263, 731, 310, 23489, 25260, 29892, 263, 1051, 310, 23489, 1169, 29892, 278, 3414, 338, 304, 6597, 278, 1226, 1169, 515, 1009, 963, 3863, 304, 5412, 515, 2, 16010, 29899, 29889, 2, 8570, 304, 278, 22999, 362, 29889, 29889, 24358, 2098, 3838, 29889, 29872, 2029, 18897, 526, 367, 367, 5545, 467, 4805, 1156, 278, 337, 28809, 467, 2, 8570, 304, 278, 265, 7492, 3620, 29889, 16743, 292, 25260, 29889, 2, 29914, 296, 663, 29871, 29901, 518, 3366, 29915, 263, 1900, 310, 306, 471, 304, 3377, 292, 29889, 2, 7058, 471, 278, 5828, 2020, 2020, 306, 748, 15007, 3377, 292, 29889, 2, 1839, 310, 1226, 1169, 29901, 2, 1839, 7058, 742, 525, 24098, 742, 525, 275,
Raw Labels (Token IDs): [[6024, 7851, 742, 15516, 13420, 2033, 13, 1839, 8143, 742, 525, 4716, 742, 525, 2033, 2], [6024, 7851, 742, 15516, 525, 29878, 431, 29890, 728, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 7851, 742, 15516, 525, 29874, 2033, 13, 1839, 6506, 3788, 303, 3096, 29879, 3788, 303, 3096, 2033, 2], [6024, 8143, 742, 525, 3166, 742, 525, 2033, 13, 1839, 6506, 742, 525, 262, 3788, 29881, 3864, 2033, 2], [6024, 6506, 742, 525, 11102, 742, 525, 275, 2033, 13, 1839, 7851, 742, 15516, 525, 974, 2033, 2], [6024, 8143, 742, 525, 1552, 742, 525, 2033, 13, 1839, 6506, 742, 525, 275, 742, 525, 598, 2033, 2]]
Sample Predictions (Decoded): ['nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic changes. dealinging sentences./entence : [[" I sun was, went very I million had had in inWhen the concert finished, I felt like a bird, freely.[\'_ edits:[\'When\', \' \'\'] [\'[\'replace\', \'con\', \' [\'insert\', \'\', \',\'] [\'insert\', \'which\', \'\']', 'nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic changes. dealinging sentences./entence : [[": theic to the can extract organ, but, sh things..To inorganic,berish, we can make bag bag, hat hat and other stuffs[\'_ edits: [\'in\', \' \'aubbish\'] [\'[\'replace\', \'\', \'a\') [\'[\'replace\', \'\', \'bag\') [\'[\'replace\', \'inuff\',\', \'stuff\'] [\'insert\', \'\', \'rubbish\'] [\'insert\', \'\', \'a\'] [\'insert\', \'\', \'a\'] [\'replace\',stuffs\',\'stuff\']', 'nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic changes. dealinging sentences./entence : [[" the, the, the are also on the differentful conditions environmentslaces. are are to get their their work. orderidays.In addition to this, people are working in very stressful workplaces and they want to leave from condition in holidays.[\'_ edits: [\'this\', \'this\', \' [\'[\'replace\', \'this\', \'during\'] [\'delete\', \'from\', \'\'] [\'replace\', \'in\',during\']', 'nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic changes. dealinging sentences./entence : [["\' a best of I was toboarding.That was the story why why I go snowboarding.[\' of edits:[\'That\', \'story\', \'is\'] [\'[\'replace\', \'\', \'the\') [\'replace\', \'was\', \'is\'] [\'insert\', \'\', \'of\']', 'nobody is a exclusive bomb ofction tool. nobody a set of atomic sentences, a list of atomicits, the task is to extract the edits from their them edit to unique from careful-. attention to thectuation.. whitespaceorder words.eloc tokens are be be considered). extra after the relocated). attention to theonetic changes. dealinging sentences./entence : [[" year not least least, I transportation people to travel things things while they not for they.Last but not the, public transport allows people to do other things which is impossible when driving.[\'_ edits:[\'Last\', \'the\', \' [\'[\'replace\', \'not\', \'are\'] [\'delete\', \'the\', \'\'] [\'replace\', \'is\', \'are\']']
Sample Labels (Decoded): ["['insert', '', ','] ['delete', 'which', '']", "['insert', '', 'rubbish'] ['insert', '', 'a'] ['insert', '', 'a'] ['replace','stuffs','stuff']", "['delete', 'from', ''] ['replace', 'in','during']", "['replace', 'was', 'is'] ['insert', '', 'of']", "['delete', 'the', ''] ['replace', 'is', 'are']"]
Processed Predictions (Sample): ["insert', '', ',']\n['insert', 'which', '']", "insert', '', 'rubbish']\n['insert', '', 'a']\n['insert', '', 'a']\n['replace',stuffs','stuff']", "delete', 'from', '']\n['replace', 'in',during']", "replace', 'was', 'is']\n['insert', '', 'of']", "delete', 'the', '']\n['replace', 'is', 'are']"]
Processed Labels (Sample): ["['insert', '', ',']\n['delete', 'which', '']", "['insert', '', 'rubbish']\n['insert', '', 'a']\n['insert', '', 'a']\n['replace','stuffs','stuff']", "['delete', 'from', '']\n['replace', 'in','during']", "['replace', 'was', 'is']\n['insert', '', 'of']", "['delete', 'the', '']\n['replace', 'is', 'are']"]
Precision: 0.0 Recall: 0.0 F1: 0.0
***** eval metrics *****
  eval_f1        = 0.0
  eval_precision = 0.0
  eval_recall    = 0.0
